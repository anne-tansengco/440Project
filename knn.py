# -*- coding: utf-8 -*-
"""440-spam-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gI-7ZCNe85O5gHQxGGTBZH8rnI1sZ-Ad

# CPTS 440: Spam Email Detector Project

Group Members: Skyllar Estill, Anne Tansengco, Emma Fletcher, and Molly Iverson

## Imports
"""

import pandas as pd
from helper import test_accuracy_with_stats
import numpy as np
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

# Data Pre-processing
# Remove all puncuation and any common english word (such as pronouns and articles). 
# A list of these words can be found in the nltk.corpus library.

def contains_letter(word):
    return any(char.isalpha() for char in word)

def is_not_stopword(word):
  stop_words = set(stopwords.words('english'))
  return word.lower() not in stop_words

def preprocess_data(email):
  array = email.split(" ")
  filtered_words = [word for word in array if contains_letter(word)]
  filtered_no_stop_words = [word for word in filtered_words if is_not_stopword(word)]
  return " ".join(filtered_no_stop_words)

df = pd.read_csv("./emails.csv")  # reads the csv with the proper column names
df['text'] = df['text'].apply(preprocess_data)  # puts the processed data back into the df. this is so we keep the labels for if the emails are spam or ham

# Split Data into Training and Testing
# Split the data 70/30 training and testing. Since the dataset has all spam first (labeled with 1), we shuffle the dataframe first. [Why 70/30?](https://scholarworks.utep.edu/cs_techrep/1209/#:~:text=Empirical%20studies%20show%20that%20the,explanation%20for%20this%20empirical%20result.)

def split_data(df):
  shuffled_df = df.sample(frac=1).reset_index(drop=True)
  total_rows = shuffled_df.shape[0]
  row_30_p = int(0.3 * total_rows)
  row_70_p = total_rows - row_30_p
  test_data = shuffled_df.head(row_30_p)
  training_data = shuffled_df.tail(row_70_p)
  return test_data,training_data

# KNN Alg
# 1. find the feq. of each word in each email for both sets of data
# 2. find the euclidean_difference between a dictionary of words from test data and from training data

def get_word_counts(text):
  word_counts = dict()
  for word in text.split():
    if word in word_counts:
      word_counts[word] += 1
    else:
      word_counts[word] = 1
  return word_counts

def euclidean_distance(test_dict, train_dict):
  total = 0
  for word in test_dict:
    # if word is a common word
    if word in train_dict:
      total += (np.square(test_dict[word] - train_dict[word]))
      del train_dict[word]
    # word just in test
    else:
      total += (np.square(test_dict[word]))
  # word just in train
  for word in train_dict:
    total += (np.square(train_dict[word]))
  sum_total = np.sum(total)
  sum_total_sqrt = np.sqrt(sum_total)
  return sum_total_sqrt

def manhattan_distance(test_dict, train_dict):
  total = 0
  for word in test_dict:
    # if word is a common word
    if word in train_dict:
      total += (test_dict[word] - train_dict[word])
      del train_dict[word]
    # word just in test
    else:
      total += test_dict[word]
  # word just in train
  for word in train_dict:
    total += train_dict[word]
  return total

# This will take in data from both datasets, the K value, and the number of test emails that are going to be tested (this is from the 30%)
def knn(training_data, test_data, K):
  return_val = []
  training_word_counts = []
  for email in training_data['text']:
    training_word_counts.append(get_word_counts(email))
  test_word_counts = []
  for email in test_data['text']:
    similarity = []
    test_word_count = get_word_counts(email)
    for index in range(len(training_data)):
      training_values = data_values(training_data)
      euclidean_dist = euclidean_distance(test_word_count, training_word_counts[index])
      similarity.append([training_values[index], euclidean_dist])
    similarity = sorted(similarity, key = lambda i:i[1])
    k_nearest_n = []
    for i in range(K):
      k_nearest_n.append(similarity[i])
    return_val.append(spam_or_ham(k_nearest_n))

  return return_val

def spam_or_ham(k_nearest_n):
  spam = 0
  ham = 0
  for value in k_nearest_n:
    if value[0] == 1:
      spam += 1
    else:
      ham += 1
  if spam > ham:
    return 1
  else:
    return 0

def data_values(data):
  values = data['spam'].tolist()  # changed from .values
  return values

test_data, training_data = split_data(df)
spam_or_ham_values_calc = knn(training_data, test_data, 11)
spam_or_ham_values_actual = data_values(test_data)
test_accuracy_with_stats(spam_or_ham_values_actual, spam_or_ham_values_calc)

# Finding optimal K value

# df = pd.read_csv("emails_smaller_set.csv")  # reads the csv with the proper column names
# df['text'] = df['text'].apply(preprocess_data)  # puts the processed data back into the df. this is so we keep the labels for if the emails are spam or ham
test_data, training_data = split_data(df)
for k in range (50):
    spam_or_ham_values_calc = knn(training_data, test_data, k)
    spam_or_ham_values_actual = data_values(test_data)
    test_accuracy_with_stats(spam_or_ham_values_actual, spam_or_ham_values_calc)