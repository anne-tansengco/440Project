# -*- coding: utf-8 -*-
"""440-spam-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gI-7ZCNe85O5gHQxGGTBZH8rnI1sZ-Ad

# CPTS 440: Spam Email Detector Project

Group Members: Skyllar Estill, Anne Tansengco, Emma Fletcher, and Molly Iverson
"""

"""# C4.5 Decision Tree Classifier

Decision Tree Based Spam Detection Algorithm Using same data preprocessing step for an equal starting point

Create the feature and label vectors  for the decision tree algorithm
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from accuracy import test_accuracy_with_stats
from preprocess import preprocess_data

df = pd.read_csv("./emails.csv")  # reads the csv with the proper column names
df['text'] = df['text'].apply(preprocess_data)  # puts the processed data back into the df. this is so we keep the labels for if the emails are spam or ham

# separate method from split_data to split into feature matrix and decision matrix
def split_x_y_data(df):
  # use CountVectorizer to create feature matrix where the columns are features and rows are the frequency of those features
  cv = CountVectorizer()
  X = cv.fit_transform(df['text']).toarray()

  # use LabelEncoder to create Y matrix with the spam values for the decision values
  le = LabelEncoder()
  Y = le.fit_transform(df['spam'])

  # use sklearn train_test_split to split X and Y into train and test in 70/30 ratio to match KNN algorithm
  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, train_size=0.7)
  return X_train, X_test, Y_train, Y_test


X_train, X_test, Y_train, Y_test = split_x_y_data(df) # split data into TRAIN (X_train, Y_train) and TEST (X_test, Y_test)

import math

# uses recursion due to the fact that trees are inherently recursive

class C45:
  def __init__(self):
    self.tree = None

  def buildTree(self, X_train, Y_train):
    # Base case: only one class exists in the tree
    if len(np.unique(Y_train))==1:
      return {'spam': Y_train[0]}

    numRows,numCols = X_train.shape

    # No more features to check out
    if numCols == 0:
      classes, classCounts = np.unique(Y_train, return_counts=True)
      if classCounts[0] > classCounts[1]:
        return {'spam': classes[0]}
      else:
        return {'spam': classes[1]}

    splitIndex, splitValue = self.highestInformationGain(X_train, Y_train, numCols)

    # No highest information to gain
    if splitIndex == None:
      classes, classCounts = np.unique(Y_train, return_counts=True)
      if classCounts[0] > classCounts[1]:
        return {'spam': classes[0]}
      else:
        return {'spam': classes[1]}

    tree = {'index':  splitIndex, 'value': splitValue, 'leftChild': {}, 'rightChild': {}}
    # All rows from specific column
    values = X_train[:, splitIndex] <= splitValue
    # based on true values above, i.e less than
    leftChild = self.buildTree(X_train[values], Y_train[values])
    #based on false values above, i.e greater than
    rightChild = self.buildTree(X_train[~values], Y_train[~values])

    tree['leftChild'] = leftChild
    tree['rightChild'] = rightChild

    return tree

  # entropy = -sum(each k in K p(k) * log(p(k)))
  # Note: Logarithm with base 2 is used in calculations according to https://sumit-kr-sharma.medium.com/understanding-c4-5-decision-tree-algorithm-3bf0981faf4f
  def calculateEntropy(self, spamLabels):
    classCounts = np.unique(spamLabels, return_counts=True)[1]
    sum=0
    for count in classCounts:
      sum += (count/len(spamLabels)) * math.log2(count/len(spamLabels))
    return -sum


  # the entropy of the parent node subtracted entropy of the weighted children nodes
  def calculateInformationGain(self, parent, child1, child2):
    parentEntropy = self.calculateEntropy(parent)
    child1Entropy = self.calculateEntropy(child1)
    child2Entropy = self.calculateEntropy(child2)

    child1Prob = len(child1)/len(parent)
    child2Prob = len(child2)/len(parent)

    weightedChild1 = child1Prob * child1Entropy
    weightedChild2 = child2Prob * child2Entropy
    return parentEntropy - weightedChild1 - weightedChild2

  # Where should we require a dicision, i.e. where do we gain the most information
  def highestInformationGain(self, X_train, Y_train, numCols):
    highestCurrent = 0
    highestIndex = -1
    highestVal = -1

    for curIndex in range(numCols):
      uniques = np.unique(X_train[:, curIndex])
      for val in uniques:
        values = X_train[:, curIndex] <= val
        # Values within the current column index that are less than or equal to val
        left_spam = Y_train[values]
        # Values within the current column index that are greater than val
        right_spam = Y_train[~values]

        info = self.calculateInformationGain(Y_train, left_spam, right_spam)

        if info>highestCurrent:
          highestCurrent = info
          highestIndex = curIndex
          highestVal = val


    return highestIndex, highestVal

  # Search the tree for each feature set of X_test
  def predictions(self, X_test):
    predictions = []
    for row in X_test:
      predictions.append(self.prediction(row, self.tree))
    return predictions

  # search the tree recursively using the left and right child trees
  def prediction(self, data, tree):
    if 'spam' in tree.keys():
      return tree['spam']
    colIndex = tree['index']
    value = tree['value']
    if data[colIndex] <= value:
      return self.prediction(data, tree['leftChild'])
    else:
      return self.prediction(data, tree['rightChild'])

# Build the decision tree and get the predictions from it

c45 = C45()
c45.tree = c45.buildTree(X_train, Y_train)

predictionVals = c45.predictions(X_test)

totalCorrect = 0
index=0
total = len(Y_test)
for val in predictionVals:
  if val == Y_test[index]:
    totalCorrect+=1
  index+=1

test_accuracy_with_stats(Y_test, predictionVals)

print(totalCorrect/total)