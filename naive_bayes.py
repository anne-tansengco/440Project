# # -*- coding: utf-8 -*-
# 440-spam-detection.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1gI-7ZCNe85O5gHQxGGTBZH8rnI1sZ-Ad

# # CPTS 440: Spam Email Detector Project

# Group Members: Skyllar Estill, Anne Tansengco, Emma Fletcher, and Molly Iverson

import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import SelectKBest, chi2

from preprocess import preprocess_data

# Naive Bayes
# Starting the algorithm off with the template from Homework 4 and adjusting the algorithm to follow the same processing as the other algorithms to ensure an equal starting point for the classifiers.

df = pd.read_csv("./emails.csv")  # reads the csv with the proper column names
df['text'] = df['text'].apply(preprocess_data)  # puts the processed data back into the df. this is so we keep the labels for if the emails are spam or ham

shuffled_df = df.sample(frac=1).reset_index(drop=True)
X_train, X_test, y_train, y_test = train_test_split(shuffled_df['text'], shuffled_df['spam'], test_size=0.3, random_state=42)

# Feature Selection
# We use CountVectorizer to transform the text data into a numerical representation. Then, we use SelectKBest with the chi-squared statistical test to select the top 100 most informative features for spam classification.

# Select features
vectorizer = CountVectorizer(lowercase=True, stop_words='english')
X_train_vectorized = vectorizer.fit_transform(X_train)

selector = SelectKBest(chi2, k=100)
X_train_selected = selector.fit_transform(X_train_vectorized, y_train)

feature_names = vectorizer.get_feature_names_out()
selected_feature_indices = selector.get_support(indices=True)
selected_feature_names = [feature_names[i] for i in selected_feature_indices]

print(selected_feature_names)

# Implementing Naive Bayes Classifier
# We define a Naive Bayes Classifier (NBC) class that trains the classifier using provided training data, computes the conditional probabilities of words given the class labels (ham or spam), and predicts the class labels for the test data. It also includes an evaluation function to assess accuracy.

class NBC:
    def __init__(self, features):
        self.features = features
        self.word_probs = {}

    def fit(self, X_train, y_train):
        # Compute P(Xi | Y) for each word and each class (ham and spam)
        num_ham = np.sum(y_train == 0)
        num_spam = np.sum(y_train == 1)
        total_ham_words = 0
        total_spam_words = 0


        for i in range(len(X_train)):
            email = X_train.iloc[i]
            label = y_train.iloc[i]

            total_words = len(email.split())

            if label == 0:
                total_ham_words += total_words
            else:
                total_spam_words += total_words

            word_count = {}
            for word in email.split():
                if word in self.features:
                    if word in word_count:
                        word_count[word] += 1
                    else:
                        word_count[word] = 1



            for word, count in word_count.items():
                if word not in self.word_probs.keys():
                    self.word_probs[word] = {0: 0, 1: 0}

                self.word_probs[word][label] += count

        # Compute probabilities P(Xi | Y)
        for word, counts in self.word_probs.items():
            self.word_probs[word][0] = (self.word_probs[word][0] + 1) / (total_ham_words + len(self.features))
            self.word_probs[word][1] = (self.word_probs[word][1] + 1) / (total_spam_words + len(self.features))


    def predict(self, X_test):
        predictions = []
        for email in X_test:
            ham_prob = 0
            spam_prob = 0
            for word in email.split():
                if word in self.features:
                    if word in self.word_probs:
                        ham_prob += np.log(self.word_probs[word][0])
                        spam_prob += np.log(self.word_probs[word][1])

            if ham_prob > spam_prob:
                predictions.append(0)
            else:
                predictions.append(1)

        return predictions


def evaluate(NBC, X_test, y_test):
    predictions = NBC.predict(X_test)
    accuracy = np.mean(predictions == y_test)
    stats = classification_report(y_test, predictions)
    c_matrix = confusion_matrix(y_test, predictions)
    return accuracy, stats, c_matrix

# Testing the algorithm
nbc = NBC(selected_feature_names)
nbc.fit(X_train, y_train)

accuracy, stats, c_matrix = evaluate(nbc, X_test, y_test)
print("ACCURACY:", accuracy)
print("STATS:", stats)
print("CONFUSION MATRIX:", c_matrix)